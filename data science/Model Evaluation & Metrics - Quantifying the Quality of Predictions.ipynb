{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Metrics - Quantifying the Quality of Predictions\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "There are three different approaches to evaluate the quality of prediction of a mode:\n",
    "\n",
    "- **Estimator Score Method**: Estimators have a *score* method providing a default evaluation criterion for the problem they are designed to solve. This info can be found in each estimator's documentation.\n",
    "\n",
    "- **Scoring Parameter**: Model-evaluation tools use ***cross-validation*** (such as *model_selection.cross_val_score* and *model_selecction.GridSearchCV*) rely on an internal scoring strategy.\n",
    "\n",
    "- ** Metric Functions**: The **metrics** module implements functions assessing prediction error for specific purposes, including *Classification Metrics*, *Multilabel ranking metrics*, *Regression metrics* and *Clustering metrics*.\n",
    "\n",
    "Additionally, ***Dummy Estimators*** are useful to get a baseline value of those metrics for random predcitions.\n",
    "\n",
    "\n",
    "## 1 The `scoring` Parameter: Defining Model Evaluation Rules\n",
    "\n",
    "This is mainly for model selection & evaluation using tools such as **`model_selection.GridSearchCV`** and **`model_selection.cross_val_score`**, take a `scoring` parameter that controls what metric they apply to the estimators evaluated.\n",
    "\n",
    "### 1.1 Common Cases: Predifined Values\n",
    "\n",
    "For the most common use cases, we can designate a scorer object with the `scoring` parameter. All possible values are listed as below. All scorer objects follow the convention that \n",
    "\n",
    "***\"higher return values are better than lower return values\"***. \n",
    "\n",
    "Thus metrics which measure the distance between the model and the data, like **`metrics.mean_squared_error`**, are available as **`neg_mean_squared_error`** which return the negated value of the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Scoring       |Function               |Comment           |\n",
    "|--|--|--|\n",
    "|**Classification**|   | |\n",
    "|'accuracy' |`metrics.accuracy_score` | |\n",
    "|'average_precision' |`metrics.average_precision_score` |It corresponds to the area under the precision-recall curve |\n",
    "|'f1' |`metrics.f1_score` |For binary targets |\n",
    "|'f1_micro' |`metrics.f1_score` | micro-averaged|\n",
    "|'f1_macro' |`metrics.f1_score` | macro-averaged|\n",
    "|'f1_weighted' |`metrics.f1_score` | weighted average |\n",
    "|'f1_samples' |`metrics.f1_score` |by multilabel sample |\n",
    "|'neg_log_loss' |`metrics.log_loss` | requires `predict_proba` support |\n",
    "|'precision' etc |`metrics.precision_score` |suffixes apply as with 'f1' |\n",
    "|'recall' etc. |`metrics.recall_scores` |suffixes apply as with 'f1' |\n",
    "|'roc_auc' |`metrics.roc_auc_score` | |\n",
    "|**Clustering**|   | |\n",
    "| 'adjusted_rand_score'|`metrics.adjusted_rand__score`| 2|\n",
    "|**Regression**|   | |\n",
    "| 'r2'| `metrics.r2_score`| |\n",
    "| 'neg_mean_absolute_error'| `metrics.mean_absolute_error`| |\n",
    "| 'neg_mean_squared_error'| `metrics.mean_squared_error`| |\n",
    "| 'neg_median_absolute_error'| `metrics.median_absolute_error`| |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05362861 -0.09135357 -0.0441815  -0.06721635 -0.14272846 -0.15222303\n",
      " -0.19022653 -0.07655709 -0.04210015 -0.04571209]\n",
      "[ 1.          0.93333333  1.          1.          1.          0.93333333\n",
      "  0.93333333  1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "iris = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "X, y = iris.iloc[:,0:4], iris.Species\n",
    "\n",
    "model_1 = svm.SVC(probability=True, random_state=0)\n",
    "print cross_val_score(model_1, X, y, scoring=\"neg_log_loss\", cv=10)\n",
    "\n",
    "model_2 = svm.SVC()\n",
    "print cross_val_score(model_2, X, y, scoring=\"accuracy\", cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list all the scorer objects by the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': make_scorer(accuracy_score),\n",
       " 'adjusted_rand_score': make_scorer(adjusted_rand_score),\n",
       " 'average_precision': make_scorer(average_precision_score, needs_threshold=True),\n",
       " 'f1': make_scorer(f1_score),\n",
       " 'f1_macro': make_scorer(f1_score, average=macro, pos_label=None),\n",
       " 'f1_micro': make_scorer(f1_score, average=micro, pos_label=None),\n",
       " 'f1_samples': make_scorer(f1_score, average=samples, pos_label=None),\n",
       " 'f1_weighted': make_scorer(f1_score, average=weighted, pos_label=None),\n",
       " 'log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       " 'mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),\n",
       " 'mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),\n",
       " 'median_absolute_error': make_scorer(median_absolute_error, greater_is_better=False),\n",
       " 'neg_log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       " 'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),\n",
       " 'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),\n",
       " 'neg_median_absolute_error': make_scorer(median_absolute_error, greater_is_better=False),\n",
       " 'precision': make_scorer(precision_score),\n",
       " 'precision_macro': make_scorer(precision_score, average=macro, pos_label=None),\n",
       " 'precision_micro': make_scorer(precision_score, average=micro, pos_label=None),\n",
       " 'precision_samples': make_scorer(precision_score, average=samples, pos_label=None),\n",
       " 'precision_weighted': make_scorer(precision_score, average=weighted, pos_label=None),\n",
       " 'r2': make_scorer(r2_score),\n",
       " 'recall': make_scorer(recall_score),\n",
       " 'recall_macro': make_scorer(recall_score, average=macro, pos_label=None),\n",
       " 'recall_micro': make_scorer(recall_score, average=micro, pos_label=None),\n",
       " 'recall_samples': make_scorer(recall_score, average=samples, pos_label=None),\n",
       " 'recall_weighted': make_scorer(recall_score, average=weighted, pos_label=None),\n",
       " 'roc_auc': make_scorer(roc_auc_score, needs_threshold=True)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.SCORERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining Your Scoring Strategy from Metric Functions\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#defining-your-scoring-strategy-from-metric-functions\n",
    "\n",
    "### 1.3 Implementing Your Own Scoring Object\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Classification Metrics\n",
    "\n",
    "**`sklearn.metrics`** module implements several loss, score, and utility functions to measure classification preformance. Some metrics may require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the *`sample_weight`* parameter.\n",
    "\n",
    "Some of these are restricted to the **binary classification case**:\n",
    "\n",
    "|                         FUNCTION|                COMMENT         |\n",
    "|                         --|                --      |\n",
    "| `matthews_corrcoef(y_true, y_pred[, ...])`|                Compute the Matthews Correlation Coefficient (MCC) for binary classes       |\n",
    "|`precision_recall_curve(y_true, probas_pred)`|compute precision-recall pairs for different probability thresholds|\n",
    "|`roc_curve(y_true, y_score[, pos_label, ...])`|Compute Receiver Operating Characteristic (ROC)|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others also work in the **multiclass case**:\n",
    "\n",
    "|                         FUNCTION|                COMMENT         |\n",
    "|                         --|                --      |\n",
    "|`cohen_kappa_score(y_1, y2[, labels, weights])`| Cohen's kappa: a statistics that measure inter-annotator agreement|\n",
    "|`confusion_matrix(y_true, y_pred[, labels, ...])`|Compute Confusion Matrix to evaluate the accuracy of a classification|\n",
    "|`hinge_loss(y_true, pred_decision[, labels, ...])`|Average hinge loss(non-regularized)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some also work in the **multilabel case**:\n",
    "\n",
    "|                         FUNCTION|                COMMENT         |\n",
    "|                         --|                --      |\n",
    "|`accuracy_score(y_true, y_pred[, normalize, ...])`|Accuracy classification score.|\n",
    "|`classficication_report(y_true, y_pred[,...])`| Build a text report showing the main classification metrics|\n",
    "|`f1_score(y_true, y_pred[, labels, ...])`|Compute the F1 score, also known as balanced F-score or F-measure|\n",
    "|`fbeta_score(y_true, y_pred, beta[, labels, ...])`|Compute the F-beta score|\n",
    "|`hamming_loss(y_true, y_pred[, labels, ...])`|compute the average Hamming loss.|\n",
    "|`jaccard_similarity_score(y_true, y_pred[,...])`|Jaccard similarity coefficient score|\n",
    "|`log_loss(y_true, y_pred[, eps, normalize, ...])`|Log loss, aka logistic loss or cross-entropy loss.|\n",
    "|`precision_recall_fscore_support(y_true, y_pred)`|Compute precision, recall, F-measure and support for each class|\n",
    "|`precision_score(y_true, y_pred[, labels, ...])`|Compute the precision|\n",
    "|`recall_score(y_true, y_pred[, labels, ...])`|Compute the recall|\n",
    "|`zero_one_loss(y_true, y_pred[, normalize, ...])`|Zero-one classification loss.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some work with **binary and multilabel (but not multiclass)** problems.\n",
    "\n",
    "|                         FUNCTION|                COMMENT         |\n",
    "|                         --|                --      |\n",
    "|`average_precision_score(y_true, y_score[,...])`|Compute average precision(AP) from predictions cores|\n",
    "|`roc_auc_score(y_true, y_score[, average, ...])`|Compute Area Under the Curve (AUC) from prediction scores|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
